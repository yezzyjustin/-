CCNet: Criss-Cross Attention for Semantic Segmentation

为了解决上述问题，我们的动机是用几个连续的稀疏连接图取代常见的单个密集连接图，这通常需要更少的计算资源。在不损失一般性的情况下，我们使用两个连续的交叉关注模块，其中每个模块对特征映射中的每个位置只有稀疏连接 (约 pn)。对于每个像素/位置，纵横交错的注意力模块在其水平和垂直方向上聚合上下文信息。通过连续叠加两个交叉的注意模块，每个位置都可以从给定图像的所有像素中收集上下文信息。上述分解策略将大大降低时间和空间的复杂性，从 O (N 2)到 O (N p N)。

我们在图 1 中比较了非局部模块[9]与我们的交叉注意模块之间的差异。具体来说，非局部模块和交叉注意模块都向输入特征图提供信息，生成每个位置的注意图，并将输入特征图转换为自适应特征图。然后，采用加权求和的方法，在基于注意图的自适应特征图中收集其他位置的上下文信息。与非局部模块采用的密集连接不同，特征中的每个位置 (如蓝色)在我们的纵横交错的注意模块中，map 与同一行同列的其他 map 是稀疏连接的，导致预测的注意 map 只有大约 2 p N 个权重，而非局部模块中的权重为 N。

为了实现捕获全图像依赖关系的目标，我们创新地简单地对交叉关注模块进行循环操作。其中，局部特征首先通过一个纵横交错的注意模块传递，在水平方向和垂直方向上收集上下文信息。然后，通过将第一个交叉关注模块生成的特征映射馈送到第二个交叉关注模块，从交叉路径中获得的额外上下文信息最终实现了所有位置的完全依赖关系。如图 1 (b)所示，第二个特征图中的每个位置 (例如红色)可以从所有其他位置收集信息，以增强位置表示。我们共享交叉模块的参数，以保持我们的模型苗条。由于输入和输出都是卷积特征图，所以交叉注意模块可以很容易地插入到任何全卷积神经网络 (称为 CCNet)中，以端到端方式学习全图像上下文信息。由于交叉关注模块的易用性，CCNet 可以直接扩展到三维网络，以获取远程时间上下文信息。

此外，为了推动所提出的循环交叉注意方法学习更多的判别特征，我们引入了一个类别一致损失来增强 CCNet。特别是，类别一致损失迫使网络将图像中的每个像素映射到特征空间中的 n 维向量，这样，属于同一类别的像素的特征向量靠得很近，而属于不同类别的像素的特征向量则相距很远。

![[Pasted image 20240615133007.png]]

**Network Architecture**
网络架构如图 2 所示。输入图像通过深度卷积神经网络 (deep convolutional neural network, DCNN)，该网络以全卷积的方式设计[10]，生成空间大小为 H × w 的特征图 X。为了保留更多的细节并有效地生成密集的特征图，我们去掉了最后两个降采样操作，并在随后的卷积层中使用扩张卷积，从而将输出特征图 X 的宽度/高度扩大到输入图像的1/8。
![[Pasted image 20240615134142.png]]
给定 X，我们首先应用卷积层获得降维特征映射 H。然后，将 H 输入到交叉关注模块中，生成一个新的特征图 H 0，该特征图将其交叉路径中每个像素的上下文信息聚合在一起。特征图 H 0 仅包含水平方向和垂直方向的上下文信息，不足以实现准确的语义分割。为了获得更丰富、更密集的上下文信息，我们将特征图 H 0 再次输入到交叉关注模块中，输出特征图 H 00。因此，H 00 中的每个位置实际上收集了所有像素的信息。前后两个交叉的注意力模块共享相同的参数，避免添加过多的额外参数。我们将这种循环结构命名为循环交叉注意 (RCCA)模块。

然后，我们将密集上下文特征 H 00 与局部表示特征 x 连接起来，然后是一个或几个具有批处理归一化的卷积层激活特征融合。最后，将融合后的特征输入到分割层中，预测最终的分割结果。

**Criss-Cross Attention**
![[Pasted image 20240615134325.png]]
![[Pasted image 20240615134342.png]]
![[Pasted image 20240615134357.png]]

![[CCNet Criss-Cross Attention for Semantic Segmentation.pdf]]