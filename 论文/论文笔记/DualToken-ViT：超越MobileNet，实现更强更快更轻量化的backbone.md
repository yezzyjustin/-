DualToken-ViT: Position-aware efficient vision transformer with dual token fusion

自注意力 (self-attention)视觉 Transformer (ViTs)已经成为计算机视觉领域一种非常有竞争力的架构。与卷积神经网络 (CNNS)不同，ViTs 能够进行全局信息共享。随着 ViTs 各种结构的发展，它们在许多视觉任务中的优势日益显现。然而，自注意力的二次复杂性使 ViTs 计算密集型，并且它们缺乏局部性和平移等变性的归纳偏见，因此需要比 CNNS 更大的模型规模才能有效学习视觉特征。

在本文中，作者提出了一种轻量级和高效的视觉 Transformer 模型，称为 DualToken-ViT。作者提出的模型具有更高效的注意力结构，旨在替代自注意力。作者结合了卷积和自注意力的优点，分别利用它们提取局部和全局信息，然后融合两者的输出，以实现高效的注意力结构。虽然窗口自注意力也能够提取局部信息，但作者观察到它在作者的轻量级模型上效率不如卷积。为了减少全局信息广播中自注意力的计算复杂性，作者通过逐步降采样来降低产生键和值的特征图，这可以在降采样过程中保留更多的信息。

图像中的关键 token 与位置感知全局 token 中相应的 token 的相关性更高。

![[Pasted image 20240316173703.png]]

模型有两个分支：图像 Token 分支和位置感知全局 Token 分支，图像 Token 分支负责从位置感知全局 Token 中获取各种信息，而位置感知全局 Token 分支负责通过图像 Token 分支更新位置感知全局 Token 并传递信息，在每个 dual token 块的注意力部分，作者从位置感知全局 token 中获取信息并融合局部和全局信息，在 FFN 后，作者还添加了 LightvIT 提出的 BiDim Attn。

<font color="#00b050">局部和全局信息的融合</font>
在每个 dual token 块的注意力部分，通过两个分支提取局部和全局信息，分别为 conv 编码器和位置感知 token 模块，然后融合这两部分。

![[Pasted image 20240316174429.png]]

作者在模型的每个块中使用 Conv 编码器来提取局部信息，因为对于轻量级模型，使用卷积提取局部信息的性能将优于窗口自注意。Conv 编码器的结构与 ConvNeXt 块相同，表示如下：
![[Pasted image 20240316174638.png]]

**位置感知 token 模块**
这个模块负责提取全局信息，结构如（b）所示，为了降低提取全局信息的复杂性，首先对包含局部信息的 Xlocal 进行降采样并聚合全局信息，然后使用位置感知全局 token 来丰富全局信息，最后将这些全局信息广播到图像 token。

