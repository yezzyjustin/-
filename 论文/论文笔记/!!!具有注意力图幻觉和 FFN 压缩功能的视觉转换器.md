Vision transformer with attention map hallucination and FFN compaction

先前的研究主要集中在设计更简单的多 head 自注意力 MHSA，一般来说，现有的关于缓解 MHSA 复杂性的方法要么选择稀疏化输入 Token，要么在计算注意力图时减少空间的和通道维度。

是否真的需要通过昂贵的 Query 和 Key 信号之间的相关计算来获得每个注意力图呢？

一般网络中的注意力图之间的平均余弦相似度超过 50％，因此，通过复杂的 Query-Key 相关计算生成的注意力图是冗余的或相关的。

FFN 占据模型中相当大的计算成本，但通常被忽视。作者提出了使用 Hallucinate 注意力图和 FFN 压缩的视觉设计。作者设计的 hMHSA，通过使用更简单的操作将一半注意力图从另一半中生成，而不是通过昂贵的 Q-K 相关计算获得所有的注意力图。此外，作者提出了紧凑型 FFN，不是直接降低扩展比例（即压缩 FFN 隐藏维度），这必然会降低模型的能力和性能，而是首先使用矩阵分解来减轻 FFN 中隐藏层到输出层的投影矩阵的计算量，以实现冗余控制，然后利用现成的重参数化技术来使其更紧凑并增强其能力。

==cFFN==
当 FFN 隐藏层的展开比为 m 时，FFN 的工作原理如下：
![[Pasted image 20240313173108.png]]
如 mobilnetv2 所示，低秩流形的非线性变换会导致信息丢失，其表现为流形中的某些点相互塌陷，这意味着模块可能会受到一些辨别点减少的影响，从而导致建模能力较差。因此为了节省计算成本，作者声明不直接降低扩展率，相反，使用 cFFN 模块来保持 M 1 和扩展比，以避免信息丢失，同时对 M 2 进行因子分解进行 FFN 压缩
![[Pasted image 20240313173418.png]]
通常，作者设 k=tmC/(m+1), t∈(0,1)，作者称 t 为 compact ratio，由于因子分解可能会损害性能，作者使用了重参数化技术，如图所示，在两个因子分解矩阵的训练阶段，使用 conv 的 r 个分支，然后是 Norm。在推理阶段，给定 conv 和 BN 的线性，r 个分支被合并。
![[Pasted image 20240313173728.png]]