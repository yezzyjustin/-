https://arxiv.org/abs/1904.04971

卷积层中的一个基本假设是，相同的卷积核应用于数据集中的每个样本，为了增加模型的容量，通常会添加更多的卷积层或增加现有卷积层大小，在这两种情况下，额外的计算成本与卷积输入的大小成比例地增加，这时的计算量可能非常大。

提出的条件参数化卷积，通过基于输入来动态计算卷积核，从而避免了传统静态卷积中所有样本共享一个卷积核的缺点，作者将 condconv 层中的卷积核参数化为 n 个专家（卷积核）的线性组合，即 $({{\alpha }_{1}}{{W}_{1}}+...+{{\alpha }_{n}}{{W}_{n}})\times \text{x}$，其中 ${\alpha }_{n}$ 是通过梯度下降学习的权重函数。

这比增加卷积核本身的大小更具计算效率，因为所有专家在每个输入中仅仅组合一次，并且这些专家组合之后只需要进行一次卷积，因此，多出来的额外计算只是计算权重和聚合卷积核的计算量。

![[Pasted image 20240311173550.png]]

