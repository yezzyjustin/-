SPFormer: Enhancing vision transformer with superpixel representation

通常，基于像素的表示将图像组织为一个规则的网格，允许 CNNs 通过滑动窗口操作提取局部详细特征。尽管 CNNS 中固有的归纳偏见 (如平移不变性)有助于它们有效地学习视觉表示，但这些网络在捕捉全局范围信息时面临挑战，通常需要堆叠多个卷积操作和/或额外的操作来扩大其感受野。

另一方面，视觉 Transformer (ViTs)将图像视为一系列 patch。这些基于 patch 的表示通常比像素表示的对应物具有更低的分辨率，能够在计算上高效地实现全局范围的自注意力操作。虽然注意力机制成功捕获了全局交互，但这也意味着在保留局部细节 (如物体边界)方面付出了代价。此外，基于 patch 的表示的低分辨率给适应高分辨率密集预测任务 (如分割和检测)带来了挑战，这些任务需要同时保留局部细节和全局上下文信息。

**是否可以同时保留局部细节和有效捕捉长程关系中受益？**
作为回应，作者探索了基于超像素的解决方案，这些解决方案在深度学习出现之前已在计算机视觉中广泛使用。这些解决方案提供了局部一致的结构，并减少了与像素级处理的计算开销。具体来说，根据输入自适应地，超像素将图像划分为不规则的区域，每个区域都聚集具有相似语义像素。这种方法允许使用少量超像素来通过自注意力建模全局交
受 ViTs 中 patch 表示固有限制的启发，作者引入了一种创新的方法，通过作者的超像素交叉注意力 (SCA)将超像素表示引入到超像素。Resulting 架构，超像素 Transformer (SPFormer)，巧妙地将局部细节保留与全局范围自注意力相结合，实现了端到端的可训练性。

<font color="#00b050">像素表示 Convolutional Neural Networks (CNNs)</font>将图像作为滑动窗口中的像素网格进行处理。自 AexNet 出现以来，CNN 一直是主导的网络选择，得益于几种设计选择，例如平移不变性和层次结构以提取多尺度特征。然而，它需要堆叠多个卷积操作来捕获长程信息，并且它无法轻松捕获全局范围信息，因为自注意力操作。
<font color="#00b050">基于 patch 表示</font>自注意力机制有效地捕获了长程信息。然而，它的计算成本是对输入 token 数量的四次方。视觉 Transformer (ViTs)通过将输入图像分成了一个 patch 序列 (patch 大小为 16 x 16)来解决这个问题。基于 patch 的表示释放了 Transformer 架构在计算机视觉中的力量，显著影响了多个视觉识别任务。由于 ViTs 中没有像 CNNs 那样的内置归纳偏差，学习 ViTs 需要特殊的训练增强，大规模数据集，更好的训练配方，或架构设计。为了减轻这个问题，一些工作利用卷积来划分图像，导致混合 CNN-Transformer 架构。与那些仅从现有 CNNS 和 ViTs 中收集知识的工作不同，作者探索了 ViTs 中的不同超像素表示。
<font color="#00b050">超像素表示</font>在深度学习时代之前，超像素是计算机视觉中最受欢迎的表示之一。Ren 和 Malik 使用局部一致的超像素对图像进行预处理，保留了后续识别任务所需结构，同时显著减少了与像素级处理的计算开销。超像素聚类方法包括基于图的方法，均值漂移，或 k-means 聚类。

![[Pasted image 20240327142001.png]]
