
https://arxiv.org/abs/2304.03282
https://github.com/dingmyu/DependencyViT

![[Pasted image 20240314134048.png]]

<font color="#00b050">Reversed Attention</font>
在计算机视觉领域中，注意力机制是一种用于提高模型性能的技术，它通过加权关注输入图像中的不同部分，从而使模型能够更好地处理复杂的视觉任务。具体来说，计算机视觉中的注意力机制可以分为两种类型: 空间注意力机制和通道注意力机制。空间注意力机制是一种用于关注图像中不同位置的机制。它通过对每个像素点进行加权，将模型的注意力集中在最重要的像素上，从而提高模型的性能。常用的空间注意力机制包括 SENet、CBAM 等。通道注意力机制是一种用于关注图像中不同通道的机制。它通过对每个通道进行加权，将模型的注意力集中在最重要的通道上，从而提高模型的性能。常用的通道注意力机制包括 Squeeze-and-Excitation 模块、SKNet 等。此外，注意力机制也可以与卷积神经网络中的不同层级结合使用，从而提高模型的性能。例如，在多尺度图像分割任务中，可以使用注意力机制来关注不同尺度下的特征图，以提高模型在不同尺度下的表现。

<font color="#00b050">Dependency Block</font>
简单地应用转置注意力矩阵并不能保证诱导出良好的依赖图。这是因为: (i)一个 token 可以关注的令牌数量是由多个注意力头控制的，因此依赖图不是唯一的。(ii) 不同子树的贡献没有很好地区分。在图像分类等一些下游任务中，前景树和背景树应该是不同的。为了解决以上问题: 我们进一步引入两个模块: head selector 和 message controller。
Head selector 的反向注意力层是一种新颖的神经运算符，可以捕获图像补丁之间的长期视觉依赖关系。它被制定为依赖关系图，在该图中，子 token 被训练为关注其 token 并按照标准化概率分布发送信息。这与传统的自我注意力形成鲜明对比，在传统的自我注意力中，代币从其他 token 那里收集信息。通过这种设计，等级自然会从反向的注意力层中浮现出来，依赖树是在不受监督的情况下逐步从叶子节点诱导到根节点。MLP 层是标准的前馈神经网络，用于处理反向注意力层的输出。它对输入要素进行非线性变换，并将它们映射到更高维度的空间。这有助于捕获输入要素之间更复杂的关系。依赖块的功能是捕获图像补丁之间的视觉依赖关系，从叶子节点诱发依赖树到根节点，并将输入特征映射到高维空间以捕获更复杂的关系。

<font color="#00b050">Dynamic Pooling based on Dependencies</font>
我们的 Dependency Block 能够学习 token 之间的动态和综合信息流以进行依赖归纳。直观地说，有了这种视觉依赖性，场景理解就可以用更少的计算量来简化，因为大部分信息都可以由几个节点表示。受此启发，作者引入了一种 Dynamic Pooling 方案，大大降低了计算成本 (即 FLOPS 和 GPU 内存)，并提出了一种轻量级模型 DependencyViT-Lite。DependencyvitLite 通过记录被修剪的节点与其父节点之间的关系来修剪收到的信息最少的叶子节点，以减少内存和资源成本，同时通过从其父节点处进行聚合来检索被修剪的节点，使其即使在移除 token 的情况下也能执行密集的预测任务。