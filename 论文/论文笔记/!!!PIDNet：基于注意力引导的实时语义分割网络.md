https://arxiv.org/pdf/2206.02066.pdf

https://github.com/XuJiacong/PIDNet

直接融合高分辨率的空间细节信息和低频的上下文信息的方法存在缺陷，容易使得细节特征被周围的上下文信息淹没，这种现象被称为 overshoot

overshoot 即超调，是控制系统中一种普遍的现象，指的是系统在达到稳态之前或之后，输出变量会超过其最终稳态值的情况。在 PID (即比例积分微分)控制器中，当反馈信号与期望值不同时，PID 控制器会根据比例、积分、微分三个部分计算出一个控制量来调整输出，从而使反馈信号逐渐接近期望值。但是在比例系数过大或系统响应过快时，控制器可能会产生超调现象，使得输出超过期望值一段时间，这可能导致系统出现震荡、不稳定等问题。

为了解决这个问题，作者将卷积神经网络和比例积分微分即 PID 控制器之间建立联系，并揭示了这种双分支网络可以等效于比例积分控制器。PIDNet 包含三个分支，分别用于解析空间细节信息，上下文信息，边界信息，同时采用边界注意力机制来指导空间细节信息分支和上下文信息分支的融合。

![[Pasted image 20240314140836.png]]

一个 PID 控制器包含三个组件：比例（P）控制器；积分（I）控制器；微分（D）控制器
- 其中，P 控制器关注当前信号，而 I 控制器则累加所有过去的信号。由于积分的惯性效应，当信号变化相反时，简单的 P! 控制器的输出会出现超调现象。因此通常会引入了 D 控制器进行调节，当信号变小时，D 分量将变为负数，并作为阻尼器减少超调现象。类似地，TBN，即双分支网络也是通过不同的卷积层来解析上下文和空间细节信息。
- 再来看看上图下半部分，相比于空间细节信息分支，上下文信息分支对局部信息的变化不太敏感。换个角度理解，便是细节信息和上下文信息分支在空间域中的行为类似于时间域中的 P (当前)和 I (所有先前)控制器。
- 由于 PI 控制器更加关注输入信号的低频部分，不能立即对信号的快速变化做出反应，因此它天然存在超调问题。而 D 控制器通过使控制输出对输入信号的变化敏感，从而减少了超调。如上图下半部分所示，即使不准确，细节信息分支仍会解析各种语义信息，而上下文信息分支则聚合低频上下文信息，类似于在语义上使用一个较大的均值滤波器。所以直接融合细节和上下文信息会导致某些细节特征丢失。因此，本文得出这么一个结论: 即 TBN 在傅里叶即频域中等价于一个 PI 控制器。【不得不说，现在发个顶会真是越来越卷啊，story 越来越高大上，下次生化环材估计也可以来个类比】

![[Pasted image 20240314140850.png]]

正如我们前面说到的，现有的双分支结构可以类比于 PI 控制器，这类控制器容易出现 overshoot 的问题。在控制系统中，一般我们会引入微分控制器进行调节，转换个思路，换到 CNN 这边，无非就是加多个分支嘛，你说是不是?
因此，为了缓解这个问题，本文在 TBN 上增加了一个辅助的导数分支 ADB，即在空间上模拟 PID 控制器，并突出高频语义信息。其中，考虑到每个 object 内部像素的语义是一致的，只有在相邻对象的边界处才会出现语义不一致，因此语义的差异仅在对象边界处为非零，所以 ADB 的目标是边界检测。遂本文建立了一种新的三分支实时语义分割体系结构即比例-积分-微分网络-- PIDNet ，如上图所示。PIDNet 拥有三个分支，具有互补的职责:
1. 比例分支负责解析和保留高分辨率特征图中的详细信息;
2. 积分分支负责聚合局部和全局的上下文信息以捕获远距离依赖 
3. 微分分支负责提取高频特征以预测边界区域。
同 DDRNet 一样，本文也采用级联残差块作为骨干网络，以更好地移植到硬件部署。此外为了实现更加高效，作者将 P、|和 D 分支的深度设置为适中、较深和较浅。因此，通过加深和加宽模型可以生成一系列 PIDNet 模型，即 PIDNet-s、PIDNet-M 和 PIDNet-L，也就是做对网络架构进行缩放啦~~~

<font color="#00b050">LOSS</font>
从图中可以看出，损失函数是一个复合函数，其由四部分组成。具体地:
首先, 作者在第一个 Pag 模块的输出处添加了语义头, 生成额外的语义损失 l 0以更好地优化整个网络。
其次，为了处理边界检测中的不平衡问题，本文使用加权二元交叉熵损失 l 1，而不是 Dice Loss, 因为这可以令网络更倾向于使用粗糙的边界来突出边界区域, 并增强小物体的特征。
紧接着，l 2和 l 3分别代表交叉熵损失，这里使用的是输出的边界头来协调语义分割和边界检测任务, 并增强 Bag 模块的功能, 因此在 l 3中采用了具有边界感知性的 CE 损失。
因此，PIDNet 的整体损失可以定义为:
Loss=À0l 0+À1l 1 +À2l 2 + À3 l 3
文中将这四个超参数分别设置为 0.4、20、1 和 1。

<font color="#00b050">Pag: Learning High-level Semantics Selectively</font>
![[Pasted image 20240314141341.png]]
Pixel-attention-guided fusion，Pag, 即像素注意力引导模块，很好理解，就是将比例和微分分支的特征利用一个注意力机制进行交互增强。
首先，作者提到了在其他语义分割网络中常用的横向连接 1 ateral connection 技术，该技术可以加强不同尺度的特征图之间的信息传递，提高模型的表达能力。而在 PIDNet 中，I 分支提供了丰富准确的语义信息，对于 P 和 D 分支的细节解析和边界检测至关重要。因此，作者将 1 分支视为其他两个分支的备用支持，并使其能够为它们提供所需的信息。此外，与 D 分支直接添加提供的特征图不同，作者为 P 分支引入了 Pag 来选择性地学习 1 分支中有用的语义特征。

<font color="#00b050">PAPPM: Fast Aggregation of Contexts</font>
![[Pasted image 20240314141642.png]]

众所周知，主要用于构建全局场景的先验信息。实现 Pyramid Pooling Module, PPM，上，PPM 就是对不同尺度的特征图进行池化操作，然后将不同尺度的池化特征图进行拼接形成本地和全局上下文的表示。说白了就是个多尺度融合。
作者认为 PPM 虽然能够很好地嵌入上下文信息，但它的计算过程无法并行化，非常耗时而且对于轻量级模型来说，PPM 包含的每个尺度的通道数太多，可能会超过这些模型的表示能力。因此，作者对 PPM 进行了修改，提出了一种可并行化的新的 PPM，叫做 Paralle 1 Aggregation PPM，PAPPM，并将其应用于 PIDNet-M 和 PIDNet-S 以保证它们的速度。对于深度模型 PIDNet-L，作者仍然选择 PPM，但减少了每个尺度的通道数，以减少计算量并提高速度。

<font color="#00b050">Bag: Balancing the Details and Context</font>
![[Pasted image 20240314141739.png]]

最后，边界注意力引导 Bag 模块的作用是利用边界特征来指导细节 (P)和上下文 (1)表示的融合，以实现更好的语义分割效果。作者指出，尽管上下文分支具有语义精度，但它在边界区域和小物体上丢失了太多的空间和几何细节，因此，PIDNet 利用细节分支来提供更好的空间细节，并强制模型在边界区域更加信任细节分支，同时利用上下文特征来填充其他区域。